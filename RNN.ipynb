{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b27b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac2536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6475367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5dc0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc30296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'pre_since_opened': (20, 9),\n",
    " 'pre_since_confirmed': (18, 8),\n",
    " 'pre_pterm': (18, 8),\n",
    " 'pre_fterm': (17, 8),\n",
    " 'pre_till_pclose': (17, 8),\n",
    " 'pre_till_fclose': (16, 8),\n",
    " 'pre_loans_credit_limit': (20, 9),\n",
    " 'pre_loans_next_pay_summ': (8, 5),\n",
    " 'pre_loans_outstanding': (6, 4),\n",
    " 'pre_loans_total_overdue': (2, 2),\n",
    " 'pre_loans_max_overdue_sum': (4, 3),\n",
    " 'pre_loans_credit_cost_rate': (14, 7),\n",
    " 'pre_loans5': (18, 8),\n",
    " 'pre_loans530': (20, 9),\n",
    " 'pre_loans3060': (10, 6),\n",
    " 'pre_loans6090': (6, 4),\n",
    " 'pre_loans90': (20, 9),\n",
    " 'is_zero_loans5': (2, 2),\n",
    " 'is_zero_loans530': (2, 2),\n",
    " 'is_zero_loans3060': (2, 2),\n",
    " 'is_zero_loans6090': (2, 2),\n",
    " 'is_zero_loans90': (2, 2),\n",
    " 'pre_util': (20, 9),\n",
    " 'pre_over2limit': (20, 9),\n",
    " 'pre_maxover2limit': (20, 9),\n",
    " 'is_zero_util': (2, 2),\n",
    " 'is_zero_over2limit': (2, 2),\n",
    " 'is_zero_maxover2limit': (2, 2),\n",
    " 'enc_paym_0': (4, 3),\n",
    " 'enc_paym_1': (4, 3),\n",
    " 'enc_paym_2': (4, 3),\n",
    " 'enc_paym_3': (4, 3),\n",
    " 'enc_paym_4': (4, 3),\n",
    " 'enc_paym_5': (4, 3),\n",
    " 'enc_paym_6': (4, 3),\n",
    " 'enc_paym_7': (4, 3),\n",
    " 'enc_paym_8': (4, 3),\n",
    " 'enc_paym_9': (4, 3),\n",
    " 'enc_paym_10': (4, 3),\n",
    " 'enc_paym_11': (5, 4),\n",
    " 'enc_paym_12': (4, 3),\n",
    " 'enc_paym_13': (4, 3),\n",
    " 'enc_paym_14': (4, 3),\n",
    " 'enc_paym_15': (4, 3),\n",
    " 'enc_paym_16': (4, 3),\n",
    " 'enc_paym_17': (4, 3),\n",
    " 'enc_paym_18': (4, 3),\n",
    " 'enc_paym_19': (4, 3),\n",
    " 'enc_paym_20': (5, 4),\n",
    " 'enc_paym_21': (4, 3),\n",
    " 'enc_paym_22': (4, 3),\n",
    " 'enc_paym_23': (4, 3),\n",
    " 'enc_paym_24': (5, 4),\n",
    " 'enc_loans_account_holder_type': (7, 5),\n",
    " 'enc_loans_credit_status': (7, 5),\n",
    " 'enc_loans_credit_type': (8, 5),\n",
    " 'enc_loans_account_cur': (4, 3),\n",
    " 'pclose_flag': (2, 2),\n",
    " 'fclose_flag': (2, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8ac27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c33ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(array: np.ndarray, max_len) -> np.ndarray:\n",
    "    output = np.zeros((max_len, 59))\n",
    "    output[:array.shape[0], :] = array\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a03af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path, is_train=True):\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    data.drop(columns=['rn'], inplace=True)\n",
    "    data = data.groupby(['id']).agg(list).agg(list, axis=\"columns\").reset_index()\n",
    "    data[0]=data[0].apply(lambda x:(np.array(x) + 1).T) ##becaouse of padding value = 0\n",
    "    \n",
    "    if is_train:\n",
    "        target = pd.read_csv('train_target.csv')    \n",
    "        data_target = data.merge(target, on=\"id\")\n",
    "        return data_target\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_folder(is_train=True):\n",
    "    \n",
    "    if is_train:\n",
    "        folder_path = 'train_data'\n",
    "    else:\n",
    "        folder_path = 'test_data'    \n",
    "    \n",
    "    dfs = []\n",
    "    dataset_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
    "    for f in tqdm(dataset_paths, total=len(dataset_paths)):\n",
    "        dfs.append(read_file(f, is_train))\n",
    "    df = pd.concat(dfs)\n",
    "            \n",
    "    df['lengths'] = df[0].apply(lambda x: x.shape[0])\n",
    "    buckets = [(0, 10), (10, 20), (20, 30), (30, 60)]\n",
    "    for start, to in buckets:\n",
    "        mini_df = df[(df['lengths'] > start) & (df['lengths'] <= to)]\n",
    "        mini_df.loc[:,0] = mini_df[0].apply(lambda x: pad_sequence(x, to))\n",
    "        mini_df = mini_df.sample(frac=1, random_state=SEED)\n",
    "        \n",
    "        if is_train:        \n",
    "            train_diveder = int(len(mini_df) * 0.7)\n",
    "            val_diveder = int(len(mini_df) * 0.9)\n",
    "            \n",
    "            train = mini_df[:train_diveder]\n",
    "            val = mini_df[train_diveder:val_diveder]\n",
    "            test = mini_df[val_diveder:]\n",
    "            \n",
    "            def get_X(df):\n",
    "                return torch.LongTensor(np.stack(df[0].values))  \n",
    "            \n",
    "            def get_y(df):\n",
    "                return torch.Tensor(df['flag'].values)\n",
    "        \n",
    "            torch.save(get_X(train), f'train_tensors/X_train_{to}.pt')\n",
    "            torch.save(get_y(train), f'train_tensors/y_train_{to}.pt')\n",
    "            \n",
    "            torch.save(get_X(val), f'train_tensors/X_val_{to}.pt')\n",
    "            torch.save(get_y(val), f'train_tensors/y_val_{to}.pt')\n",
    "            \n",
    "            torch.save(get_X(test), f'train_tensors/X_test_{to}.pt')\n",
    "            torch.save(get_y(test), f'train_tensors/y_test_{to}.pt')\n",
    "            \n",
    "        else:\n",
    "              \n",
    "            X = torch.LongTensor(np.stack(mini_df[0].values))\n",
    "            Id = torch.Tensor(mini_df['id'].values)\n",
    "            \n",
    "            torch.save(X, f'test_tensors/X_{to}.pt')\n",
    "            torch.save(Ids, f'test_tensors/ids_{to}.pt')\n",
    "\n",
    "\n",
    "            \n",
    "class IterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, batch_size, mode):\n",
    "        super(IterableDataset).__init__()\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.sets = [10, 20, 30, 60]        \n",
    "        self.folder_path = 'train_tensors'\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for padding_set in self.sets:\n",
    "            X = torch.load(f'{self.folder_path}/X_{self.mode}_{padding_set}.pt')\n",
    "            y = torch.load(f'{self.folder_path}/y_{self.mode}_{padding_set}.pt')\n",
    "            \n",
    "            batches_count = X.shape[0] // self.batch_size\n",
    "            \n",
    "            for _X, _y in zip(torch.tensor_split(X, batches_count, dim=0), torch.tensor_split(y, batches_count, dim=0)):\n",
    "                yield _X, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7735060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_projection(cardinality, embed_size, add_missing=True):\n",
    "    add_missing = 1 if add_missing else 0\n",
    "    return torch.nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size)\n",
    "\n",
    "def get_embedding_projection():    \n",
    "    data = pd.read_parquet('train_data/train_data_00.pq')[:10]\n",
    "    features = data.columns.tolist()\n",
    "    features.remove('id')\n",
    "    features.remove('rn')\n",
    "    embedding_projection = [create_embedding_projection(*mapping[e]) for e in features]\n",
    "    embedding_general_output_size = sum([mapping[e][1] for e in features])\n",
    "    return embedding_projection, embedding_general_output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46830d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, enc_hid_dim):\n",
    "        super().__init__()        \n",
    "        embedding_projection, embedding_general_output_size = get_embedding_projection()\n",
    "        self.embeddings = torch.nn.ModuleList(embedding_projection)\n",
    "        self.credits_rnn = torch.nn.GRU(embedding_general_output_size, enc_hid_dim, batch_first=True, bidirectional =True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc = torch.nn.Linear(in_features=enc_hid_dim * 2, out_features=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        embeddings =  [emb(tensor) for emb, tensor in zip(self.embeddings, torch.tensor_split(X, 59, dim=-1))]\n",
    "        concatted_emb = torch.squeeze(torch.cat(embeddings, dim=-1))\n",
    "\n",
    "        rnn_output, hidden_credits_rnn = self.credits_rnn(concatted_emb)\n",
    "\n",
    "        rnn_stack = torch.cat([hidden_credits_rnn[0],hidden_credits_rnn[1]], dim=-1) #torch.Size([931, 400])\n",
    "        output = self.fc(self.relu(rnn_stack))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8f7fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "import logging\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(x))\n",
    "        loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(output, torch.squeeze(y)).mean()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(x))\n",
    "        y = torch.squeeze(y)\n",
    "        \n",
    "        return output, y\n",
    "        self.validation_outputs.append((output, y))\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for output, y in validation_step_outputs:\n",
    "            y_true.append(y)\n",
    "            y_pred.append(torch.sigmoid(output))\n",
    "        self.log(\"val_loss\", AUROC()(torch.cat(y_pred), torch.cat(y_true).long()).item(), on_epoch=True)        \n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(x))\n",
    "        y = torch.squeeze(y)\n",
    "        \n",
    "        return output, y\n",
    "        self.validation_outputs.append((output, y))\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for output, y in test_step_outputs:\n",
    "            y_true.append(y)\n",
    "            y_pred.append(torch.sigmoid(output))\n",
    "        self.log(\"test_loss\", AUROC()(torch.cat(y_pred), torch.cat(y_true).long()).item(), on_epoch=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7b80846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 555 K \n",
      "--------------------------------\n",
      "555 K     Trainable params\n",
      "0         Non-trainable params\n",
      "555 K     Total params\n",
      "2.222     Total estimated model params size (MB)\n",
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1933: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.483\n",
      "Monitored metric val_loss did not improve in the last 2 records. Best score: 0.483. Signaling Trainer to stop.\n",
      "Restoring states from the checkpoint path at C:\\github\\credit_scoring\\lightning_logs\\version_36\\checkpoints\\epoch=0-step=2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at C:\\github\\credit_scoring\\lightning_logs\\version_36\\checkpoints\\epoch=0-step=2.ckpt\n",
      "C:\\Users\\kamus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.5133928060531616\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5133928060531616}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(200)\n",
    "module = LightningModule(model)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\",mode=\"min\", verbose=True, patience = 2)\n",
    "\n",
    "trainer = pl.Trainer(callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "                     max_epochs=10,\n",
    "                     deterministic=True,\n",
    "                     enable_progress_bar = False,\n",
    "                     accelerator=\"gpu\",\n",
    "                     limit_train_batches=2,\n",
    "                     limit_val_batches=2)\n",
    "trainer.fit(model=module, train_dataloaders=torch.utils.data.DataLoader(IterableDataset(1024, 'train')),\n",
    "            val_dataloaders=torch.utils.data.DataLoader(IterableDataset(1024, 'val')))\n",
    "\n",
    "trainer.test(dataloaders=torch.utils.data.DataLoader(IterableDataset(128, 'test')), ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "indexes=[]\n",
    "for x, Id in get_uploaded_tensors(False):\n",
    "    x = x.to(device)\n",
    "    score=enc.forward(x)\n",
    "    sigm = torch.sigmoid(score)\n",
    "    for pred, _id in zip (np.squeeze(sigm.detach().cpu().numpy()), Id.detach().cpu().numpy()):\n",
    "        preds.append(pred)\n",
    "        indexes.append(_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'id':[int(e) for e in indexes], 'score':preds}).sort_values(by=['id']).to_csv('1_epoch.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251c270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f336ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da65006",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    X = torch.LongTensor(row[0])\n",
    "    X = X.to(device)\n",
    "    lengths = torch.LongTensor([row['lengths']])\n",
    "    score = enc.forward(X, lengths)\n",
    "    sigm = torch.sigmoid(score).item()\n",
    "    del X\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ad8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.apply(lambda row: predict(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f47e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5eba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'score']].to_csv('1_epoch.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
