{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fadd830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c75ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e236cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ca8807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1ed3eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7eaa9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'pre_since_opened': (20, 9),\n",
    " 'pre_since_confirmed': (18, 8),\n",
    " 'pre_pterm': (18, 8),\n",
    " 'pre_fterm': (17, 8),\n",
    " 'pre_till_pclose': (17, 8),\n",
    " 'pre_till_fclose': (16, 8),\n",
    " 'pre_loans_credit_limit': (20, 9),\n",
    " 'pre_loans_next_pay_summ': (8, 5),\n",
    " 'pre_loans_outstanding': (6, 4),\n",
    " 'pre_loans_total_overdue': (2, 2),\n",
    " 'pre_loans_max_overdue_sum': (4, 3),\n",
    " 'pre_loans_credit_cost_rate': (14, 7),\n",
    " 'pre_loans5': (18, 8),\n",
    " 'pre_loans530': (20, 9),\n",
    " 'pre_loans3060': (10, 6),\n",
    " 'pre_loans6090': (6, 4),\n",
    " 'pre_loans90': (20, 9),\n",
    " 'is_zero_loans5': (2, 2),\n",
    " 'is_zero_loans530': (2, 2),\n",
    " 'is_zero_loans3060': (2, 2),\n",
    " 'is_zero_loans6090': (2, 2),\n",
    " 'is_zero_loans90': (2, 2),\n",
    " 'pre_util': (20, 9),\n",
    " 'pre_over2limit': (20, 9),\n",
    " 'pre_maxover2limit': (20, 9),\n",
    " 'is_zero_util': (2, 2),\n",
    " 'is_zero_over2limit': (2, 2),\n",
    " 'is_zero_maxover2limit': (2, 2),\n",
    " 'enc_paym_0': (4, 3),\n",
    " 'enc_paym_1': (4, 3),\n",
    " 'enc_paym_2': (4, 3),\n",
    " 'enc_paym_3': (4, 3),\n",
    " 'enc_paym_4': (4, 3),\n",
    " 'enc_paym_5': (4, 3),\n",
    " 'enc_paym_6': (4, 3),\n",
    " 'enc_paym_7': (4, 3),\n",
    " 'enc_paym_8': (4, 3),\n",
    " 'enc_paym_9': (4, 3),\n",
    " 'enc_paym_10': (4, 3),\n",
    " 'enc_paym_11': (5, 4),\n",
    " 'enc_paym_12': (4, 3),\n",
    " 'enc_paym_13': (4, 3),\n",
    " 'enc_paym_14': (4, 3),\n",
    " 'enc_paym_15': (4, 3),\n",
    " 'enc_paym_16': (4, 3),\n",
    " 'enc_paym_17': (4, 3),\n",
    " 'enc_paym_18': (4, 3),\n",
    " 'enc_paym_19': (4, 3),\n",
    " 'enc_paym_20': (5, 4),\n",
    " 'enc_paym_21': (4, 3),\n",
    " 'enc_paym_22': (4, 3),\n",
    " 'enc_paym_23': (4, 3),\n",
    " 'enc_paym_24': (5, 4),\n",
    " 'enc_loans_account_holder_type': (7, 5),\n",
    " 'enc_loans_credit_status': (7, 5),\n",
    " 'enc_loans_credit_type': (8, 5),\n",
    " 'enc_loans_account_cur': (4, 3),\n",
    " 'pclose_flag': (2, 2),\n",
    " 'fclose_flag': (2, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c09ab58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(array: np.ndarray, max_len) -> np.ndarray:\n",
    "    output = np.zeros((max_len, 59), dtype=np.int32)\n",
    "    output[:array.shape[0], :] = array\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e57bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def read_file(file_path, is_train=True):\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    data.drop(columns=['rn'], inplace=True)\n",
    "    data = data.groupby(['id']).agg(list).agg(list, axis=\"columns\").reset_index()\n",
    "    data[0]=data[0].apply(lambda x:(np.array(x, dtype=np.int32) + 1).T)\n",
    "    \n",
    "    if is_train:\n",
    "        target = pd.read_csv('train_target.csv')    \n",
    "        data_target = data.merge(target, on=\"id\")\n",
    "        return data_target\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_bucket(length):\n",
    "    buckets = [(0, 10), (10, 20), (20, 30), (30, 60)]\n",
    "    for borders in buckets:\n",
    "        if length > borders[0] and length <= borders[1]:\n",
    "            return borders[1]\n",
    "\n",
    "def create_tensors(batches_bucket, is_train):\n",
    "    X = torch.LongTensor([b[0] for b in batches_bucket])\n",
    "    if is_train:\n",
    "        y = torch.FloatTensor([b[1] for b in batches_bucket])\n",
    "        return X, y\n",
    "    else:\n",
    "        ids = torch.LongTensor([b[1] for b in batches_bucket])\n",
    "        return X, ids\n",
    "\n",
    "def split_to_batches(df, batch_size, is_train):\n",
    "    batches = {}\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        bucket =  row['bucket']\n",
    "        if bucket not in batches:\n",
    "            batches[bucket] = []\n",
    "        if is_train:\n",
    "            batches[bucket].append((row[0], row['flag']))\n",
    "        else:\n",
    "            batches[bucket].append((row[0],row['id']))\n",
    "            \n",
    "        \n",
    "        if len(batches[bucket]) == batch_size:\n",
    "            yield create_tensors(batches[bucket], is_train)\n",
    "            batches[bucket] = []\n",
    "            \n",
    "    for _, batches_bucket in batches.items():\n",
    "        if len(batches_bucket) > 0:\n",
    "            yield create_tensors(batches_bucket, is_train)\n",
    "\n",
    "    \n",
    "def write_folder(batch_size, is_train=True):\n",
    "    \n",
    "    if is_train:\n",
    "        folder_path = 'train_data'\n",
    "        folder_out_path = 'train_tensors'\n",
    "    else:\n",
    "        folder_path = 'test_data'\n",
    "        folder_out_path = 'test_tensors'\n",
    "        \n",
    "    \n",
    "    dfs = []\n",
    "    dataset_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
    "    for f in tqdm(dataset_paths, total=len(dataset_paths)):\n",
    "        dfs.append(read_file(f, is_train))\n",
    "    df = pd.concat(dfs)\n",
    "            \n",
    "    df['lengths'] = df[0].apply(lambda x: x.shape[0])\n",
    "    df['bucket'] = df['lengths'].apply(lambda x: get_bucket(x))\n",
    "    df[0] = df.apply(lambda row: pad_sequence(row[0], row['bucket']), axis=1)\n",
    "    \n",
    "    if is_train:\n",
    "        skf = StratifiedKFold(n_splits=3)\n",
    "        for fold_number, (train_index, test_index) in tqdm(enumerate(skf.split(df['id'], df['flag']))):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "\n",
    "            for i, (X, y) in enumerate(split_to_batches(train, batch_size)):\n",
    "                torch.save(X, f'{folder_out_path}/fold_{fold_number}/train/{i}_X.pt')\n",
    "                torch.save(y, f'{folder_out_path}/fold_{fold_number}/train/{i}_y.pt')\n",
    "\n",
    "            for i, (X, y) in enumerate(split_to_batches(test, batch_size)):\n",
    "                torch.save(X, f'{folder_out_path}/fold_{fold_number}/test/{i}_X.pt')\n",
    "                torch.save(y, f'{folder_out_path}/fold_{fold_number}/test/{i}_y.pt')\n",
    "    else:\n",
    "        for i, (X, ids) in enumerate(split_to_batches(df, batch_size, False)):\n",
    "            torch.save(X, f'{folder_out_path}/{i}_X.pt')\n",
    "            torch.save(ids, f'{folder_out_path}/{i}_ids.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac265eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce084286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [08:28<00:00, 254.25s/it]\n"
     ]
    }
   ],
   "source": [
    "write_folder(2048, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "149e1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "class FoldDataset(Dataset):\n",
    "    def __init__(self, fold_number, fold_type, is_train):\n",
    "        self.folder_path = f'train_tensors/fold_{fold_number}/{fold_type}' if is_train else 'test_tensors'\n",
    "        self.len = len(os.listdir(self.folder_path)) // 2\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def load(self, idx, tensor_type):\n",
    "        return torch.load(f'{self.folder_path}/{idx}_{tensor_type}.pt', map_location=device)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.load(idx, 'X'), self.load(idx, 'y')\n",
    "        else:\n",
    "            return self.load(idx, 'X'), self.load(idx, 'ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a45293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, IterableDataset\n",
    "class FoldIterDataset(IterableDataset):\n",
    "    def __init__(self, fold_number, fold_type):\n",
    "        self.folder_path = f'train_tensors/fold_{fold_number}/{fold_type}'\n",
    "        self.len = len(os.listdir(self.folder_path)) // 2\n",
    "        \n",
    "    def load(self, idx, tensor_type):\n",
    "        return torch.load(f'{self.folder_path}/{idx}_{tensor_type}.pt', map_location=device)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(self.len):\n",
    "            x = self.load(i, 'X')\n",
    "            y = self.load(i, 'y')\n",
    "            for X, Y in zip(torch.tensor_split(x, 16),torch.tensor_split(y, 16)):\n",
    "                yield X, Y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8dd90dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_fold_dataloaders(is_train):\n",
    "    if is_train:\n",
    "        for i in range(3):\n",
    "            yield DataLoader(FoldDataset(i, 'train', is_train), batch_size=None), DataLoader(FoldDataset(i, 'test', is_train), batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cf6bd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "        \n",
    "    def __init__(self, n_head=6, num_layers = 6, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        d_model=258\n",
    "        enc_hid_dim = 205\n",
    "        embedding_projection, embedding_general_output_size = get_embedding_projection()\n",
    "        self.embeddings = torch.nn.ModuleList(embedding_projection)\n",
    "        self.pos_encoder = nn.Embedding(enc_hid_dim, d_model)\n",
    "        \n",
    "        self.credits_rnn = torch.nn.GRU(embedding_general_output_size, enc_hid_dim, batch_first=True)\n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers =6)\n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings =  [emb(tensor) for emb, tensor in zip(self.embeddings, torch.tensor_split(x, 59, dim=-1))]\n",
    "        concatted_emb = torch.squeeze(torch.cat(embeddings, dim=-1))\n",
    "        \n",
    "        rnn_output, hidden_credits_rnn = self.credits_rnn(concatted_emb)\n",
    "        rnn_stack = hidden_credits_rnn[0]\n",
    "        \n",
    "        print(rnn_stack.shape)\n",
    "        #positions = torch.arange(0, 258).expand(x.shape[0], 258)#.to(device)\n",
    "        \n",
    "        print(positions.shape)\n",
    "        x = self.pos_encoder(positions) + rnn_stack\n",
    "        print(x.shape)\n",
    "        return\n",
    "        \n",
    "        x = self.pos_encoder(concatted_emb)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)        \n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8e83f2da",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [228]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "TransformerModel().to(device)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b2f6cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 10, 59])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a228d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_projection(cardinality, embed_size, add_missing=True):\n",
    "    add_missing = 1 if add_missing else 0\n",
    "    return torch.nn.Embedding(num_embeddings=cardinality+add_missing, embedding_dim=embed_size)\n",
    "\n",
    "def get_embedding_projection():    \n",
    "    data = pd.read_parquet('train_data/train_data_00.pq')[:10]\n",
    "    features = data.columns.tolist()\n",
    "    features.remove('id')\n",
    "    features.remove('rn')\n",
    "    embedding_projection = [create_embedding_projection(*mapping[e]) for e in features]\n",
    "    embedding_general_output_size = sum([mapping[e][1] for e in features])\n",
    "    return embedding_projection, embedding_general_output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd9189c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, enc_hid_dim):\n",
    "        super().__init__()\n",
    "        embedding_projection, embedding_general_output_size = get_embedding_projection()\n",
    "        self.embeddings = torch.nn.ModuleList(embedding_projection)\n",
    "        self.credits_rnn = torch.nn.GRU(embedding_general_output_size, enc_hid_dim, batch_first=True, bidirectional =True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc = torch.nn.Linear(in_features=enc_hid_dim * 2, out_features=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        embeddings =  [emb(tensor) for emb, tensor in zip(self.embeddings, torch.tensor_split(X, 59, dim=-1))]\n",
    "        concatted_emb = torch.squeeze(torch.cat(embeddings, dim=-1))\n",
    "\n",
    "        rnn_output, hidden_credits_rnn = self.credits_rnn(concatted_emb)\n",
    "\n",
    "        rnn_stack = torch.cat([hidden_credits_rnn[0],hidden_credits_rnn[1]], dim=-1) #torch.Size([931, 400])\n",
    "        output = self.fc(self.relu(rnn_stack))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c9ebb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import AUROC\n",
    "import logging\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.weights = torch.Tensor([27]).to(device)\n",
    "        self.metric = AUROC(average='weighted')\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=self.weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(torch.squeeze(x)))\n",
    "        loss = self.loss(output, torch.squeeze(y)).mean()\n",
    "        return loss    \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = torch.squeeze(self.model.forward(x))\n",
    "        y = torch.squeeze(y)\n",
    "          \n",
    "        return torch.row_stack([output, y])\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):        \n",
    "\n",
    "        catted = torch.cat(test_step_outputs, dim=1)\n",
    "        self.log(\"test_loss\", self.metric(catted[0], catted[1].long()).item())\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, ids = batch\n",
    "        output = torch.sigmoid(torch.squeeze(self.model.forward(torch.squeeze(x))))\n",
    "        \n",
    "        return dict([(a.item(), b.item()) for a,b in zip(ids.detach().cpu(), output.detach().cpu())])\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41b99ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2aeb777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(hidden_size, epoch, train_dataloader):\n",
    "    model = Model(hidden_size)\n",
    "    module = LightningModule(model)\n",
    "    callbacks=[RichProgressBar(leave=True)]\n",
    "    trainer = pl.Trainer(callbacks=callbacks,\n",
    "                         max_epochs=epoch,\n",
    "                         deterministic=True,\n",
    "                         enable_progress_bar=True,\n",
    "                         accelerator=\"gpu\")\n",
    "    trainer.fit(model=module, train_dataloaders=train_dataloader)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d08575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_on_fold(hidden_size, epoch, train_dataloader, test_dataloader):\n",
    "    trainer = fit_model(hidden_size, epoch, train_dataloader, test_dataloader)\n",
    "\n",
    "    trainer.test(dataloaders=test_dataloader)\n",
    "    return trainer.callback_metrics[\"test_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05163858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 5, 400, 20)\n",
    "    epoch = trial.suggest_int(\"epoch\", 1, 2)##############\n",
    "    losses = []\n",
    "    \n",
    "    for train_dataloader, test_dataloader in get_fold_dataloaders():\n",
    "        \n",
    "        loss = objective_on_fold(hidden_size, epoch, train_dataloader, test_dataloader)\n",
    "        losses.append(loss) \n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb3299df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "#optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"transformer\"\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ffd00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_study = optuna.load_study(study_name=\"example-study\", storage=f\"sqlite:///example-study.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7a267b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 5, 'hidden_size': 205}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e6cf9a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 4   </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">978/978</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:32 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">10.45it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.444 v_num: 256 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 4   \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m978/978\u001b[0m \u001b[38;5;245m0:01:32 • 0:00:00\u001b[0m \u001b[38;5;249m10.45it/s\u001b[0m \u001b[37mloss: 0.444 v_num: 256 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainers = []\n",
    "for train_dataloader, test_dataloader in get_fold_dataloaders(True):\n",
    "    trainers.append(fit_model(205, 5, train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ed018fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">246/246</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:19 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">12.71it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m246/246\u001b[0m \u001b[38;5;245m0:00:19 • 0:00:00\u001b[0m \u001b[38;5;249m12.71it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl = DataLoader(FoldDataset(0,'', False), batch_size=None)\n",
    "\n",
    "predictions = []\n",
    "for trainer in trainers:\n",
    "    predictions.append(trainer.predict(dataloaders=[dl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8acb9e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4dbd6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for _list in predictions:\n",
    "    for _dict in _list:\n",
    "        #print(_dict)\n",
    "        for key,value in _dict.items():\n",
    "            if key not in results:\n",
    "                results[key] = 0\n",
    "            results[key] += value / 3.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c2eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def train():\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "086585b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results.items(), columns=['id', 'score']).sort_values(by=['id']).to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba458f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8541274",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'id':[int(e) for e in ids], 'score':prediction}).sort_values(by=['id']).to_csv('rnn_weight_27.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422a777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac28671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    X = torch.LongTensor(row[0])\n",
    "    X = X.to(device)\n",
    "    lengths = torch.LongTensor([row['lengths']])\n",
    "    score = enc.forward(X, lengths)\n",
    "    sigm = torch.sigmoid(score).item()\n",
    "    del X\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001b5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee705af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = df.apply(lambda row: predict(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'score']].to_csv('1_epoch.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
